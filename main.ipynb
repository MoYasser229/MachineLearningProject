{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import nltk\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\moham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"EronDataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [subject, :, enron, methanol, ;, meter, #, :, ...\n",
      "1       [subject, :, hpl, nom, for, january, 9, ,, 200...\n",
      "2       [subject, :, neon, retreat, ho, ho, ho, ,, we,...\n",
      "3       [subject, :, photoshop, ,, windows, ,, office,...\n",
      "4       [subject, :, re, :, indian, springs, this, dea...\n",
      "                              ...                        \n",
      "5166    [subject, :, put, the, 10, on, the, ft, the, t...\n",
      "5167    [subject, :, 3, /, 4, /, 2000, and, following,...\n",
      "5168    [subject, :, calpine, daily, gas, nomination, ...\n",
      "5169    [subject, :, industrial, worksheets, for, augu...\n",
      "5170    [subject, :, important, online, banking, alert...\n",
      "Name: text, Length: 5171, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenized_messages = data['text'].str.lower().apply(word_tokenize)\n",
    "print(tokenized_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [subject, enron, methanol, meter, this, is, a,...\n",
      "1       [subject, hpl, nom, for, january, see, attache...\n",
      "2       [subject, neon, retreat, ho, ho, ho, we, re, a...\n",
      "3       [subject, photoshop, windows, office, cheap, m...\n",
      "4       [subject, re, indian, springs, this, deal, is,...\n",
      "                              ...                        \n",
      "5166    [subject, put, the, on, the, ft, the, transpor...\n",
      "5167    [subject, and, following, noms, hpl, can, t, t...\n",
      "5168    [subject, calpine, daily, gas, nomination, jul...\n",
      "5169    [subject, industrial, worksheets, for, august,...\n",
      "5170    [subject, important, online, banking, alert, d...\n",
      "Name: text, Length: 5171, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def alpha(tokens):\n",
    "    alpha = []\n",
    "    for token in tokens:\n",
    "        if str.isalpha(token) or token in ['n\\'t','won\\'t']:\n",
    "            if token == 'n\\'t':\n",
    "                alpha.append('not')\n",
    "                continue\n",
    "            elif token == 'won\\'t':\n",
    "                alpha.append('wont')\n",
    "                continue\n",
    "            alpha.append(token)\n",
    "    return alpha\n",
    "\n",
    "tokenized_messages = tokenized_messages.apply(alpha)\n",
    "print(tokenized_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [subject, enron, methanol, meter, follow, note...\n",
      "1       [subject, hpl, nom, january, see, attached, fi...\n",
      "2       [subject, neon, retreat, ho, ho, ho, around, w...\n",
      "3       [subject, photoshop, windows, office, cheap, m...\n",
      "4       [subject, indian, springs, deal, book, teco, p...\n",
      "                              ...                        \n",
      "5166    [subject, put, ft, transport, volumes, decreas...\n",
      "5167    [subject, following, noms, hpl, take, extra, m...\n",
      "5168    [subject, calpine, daily, gas, nomination, jul...\n",
      "5169    [subject, industrial, worksheets, august, acti...\n",
      "5170    [subject, important, online, banking, alert, d...\n",
      "Name: text, Length: 5171, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    no_stop = []\n",
    "    for token in tokens:\n",
    "        if token not in stopwords.words('english'):\n",
    "            no_stop.append(token)\n",
    "    return no_stop\n",
    "tokenized_messages = tokenized_messages.apply(remove_stop_words)\n",
    "print(tokenized_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       subject enron methanol meter follow note gave ...\n",
      "1       subject hpl nom january see attached file hpln...\n",
      "2       subject neon retreat ho ho ho around wonderful...\n",
      "3       subject photoshop window office cheap main tre...\n",
      "4       subject indian spring deal book teco pvr reven...\n",
      "                              ...                        \n",
      "5166    subject put ft transport volume decreased cont...\n",
      "5167    subject following noms hpl take extra mmcf wee...\n",
      "5168    subject calpine daily gas nomination julie men...\n",
      "5169    subject industrial worksheet august activity a...\n",
      "5170    subject important online banking alert dear va...\n",
      "Name: text, Length: 5171, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for token in tokens:\n",
    "        lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "tokenized_messages = tokenized_messages.apply(lemmatize)\n",
    "print(tokenized_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5682)\t0.15404804950329298\n",
      "  (0, 5119)\t0.16684788059401068\n",
      "  (0, 19572)\t0.21212760464058913\n",
      "  (0, 2765)\t0.24580013172945345\n",
      "  (0, 352)\t0.22166431330287345\n",
      "  (0, 16819)\t0.08319260674607103\n",
      "  (0, 1)\t0.10219606034575256\n",
      "  (0, 801)\t0.2712724151456959\n",
      "  (0, 22345)\t0.17866567260923577\n",
      "  (0, 39303)\t0.1877205765921573\n",
      "  (0, 41982)\t0.1519475499877675\n",
      "  (0, 44269)\t0.060022938163181314\n",
      "  (0, 13782)\t0.13829305858675012\n",
      "  (0, 5546)\t0.11636556812018235\n",
      "  (0, 26889)\t0.14246337705560963\n",
      "  (0, 18780)\t0.1001168326455955\n",
      "  (0, 29671)\t0.16118303780810428\n",
      "  (0, 16660)\t0.15383251377127008\n",
      "  (0, 40927)\t0.2054275177069678\n",
      "  (0, 5128)\t0.26203692732535905\n",
      "  (0, 9098)\t0.16841189266960738\n",
      "  (0, 39824)\t0.07680294838626842\n",
      "  (0, 35783)\t0.1619790704555782\n",
      "  (0, 36795)\t0.1519475499877675\n",
      "  (0, 5816)\t0.11783699478368746\n",
      "  :\t:\n",
      "  (4134, 43220)\t0.2242773747784929\n",
      "  (4134, 4722)\t0.16865382007384766\n",
      "  (4134, 7637)\t0.1998046431770514\n",
      "  (4134, 22667)\t0.12855307181182754\n",
      "  (4134, 21022)\t0.06814013228586584\n",
      "  (4134, 29534)\t0.07965463973259265\n",
      "  (4134, 5546)\t0.22123228516219245\n",
      "  (4134, 26889)\t0.13542450299982337\n",
      "  (4134, 18780)\t0.09517022959278777\n",
      "  (4134, 39824)\t0.07300824484928169\n",
      "  (4134, 5816)\t0.05600743427324676\n",
      "  (4134, 39832)\t0.047403644029115746\n",
      "  (4134, 38671)\t0.034933044142126\n",
      "  (4135, 21909)\t0.7135648915183144\n",
      "  (4135, 29453)\t0.22446728104513328\n",
      "  (4135, 18303)\t0.17905288103349168\n",
      "  (4135, 43928)\t0.36794367848619314\n",
      "  (4135, 21902)\t0.14040486943995864\n",
      "  (4135, 23820)\t0.21973569132328044\n",
      "  (4135, 36389)\t0.14527080270982662\n",
      "  (4135, 6803)\t0.1521467629986378\n",
      "  (4135, 89)\t0.3568413986791272\n",
      "  (4135, 1015)\t0.15746070010970048\n",
      "  (4135, 18780)\t0.07483272986663478\n",
      "  (4135, 38671)\t0.05493598296215625\n"
     ]
    }
   ],
   "source": [
    "x = data['text']\n",
    "y = data['label_num']\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2, random_state=34, stratify=y)\n",
    "\n",
    "# Create the tf-idf vectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='ascii')\n",
    "\n",
    "# First fit the vectorizer with our training set\n",
    "tfidf_train = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Now we can fit our test data with the same vectorizer\n",
    "tfidf_test = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[718  17]\n",
      " [  7 293]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       735\n",
      "           1       0.95      0.98      0.96       300\n",
      "\n",
      "    accuracy                           0.98      1035\n",
      "   macro avg       0.97      0.98      0.97      1035\n",
      "weighted avg       0.98      0.98      0.98      1035\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression().fit(tfidf_train, y_train)\n",
    "pred_results = clf.predict(tfidf_test)\n",
    "print(\"Confusion Matrix\" + confusion_matrix(y_test,pred_results))\n",
    "print(classification_report(y_test,pred_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70f5df420721fc3aa19f20d0a5504125503fd3fc5da58dc6d03769338461bb36"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
